{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1FYhWUKX-zM"
   },
   "source": [
    "# Loading and Displaying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIKoEd4kGZvF",
    "outputId": "2ec49b2c-5cc1-43db-d866-d1cfbe737899"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "real_news = pd.read_csv('True.csv')\n",
    "\n",
    "# Printing head and tail\n",
    "print(real_news)\n",
    "\n",
    "# Print length => rows\n",
    "print(\"Length (rows):\", len(real_news))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTBA9Fg9GiPq",
    "outputId": "e21f6e62-c4e6-415b-be19-562cd86e8db3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "fake_news = pd.read_csv('Fake.csv')\n",
    "\n",
    "# Printing head and tail\n",
    "print(fake_news)\n",
    "\n",
    "# Print length => rows\n",
    "print(\"Length (rows):\", len(fake_news))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvEKkMnwYCoO"
   },
   "source": [
    "# Combining and Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "rADmIBu0Gj-g",
    "outputId": "d8ee8aff-75aa-4185-d136-81f943ca5273"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "fake_news = pd.read_csv('Fake.csv')\n",
    "real_news = pd.read_csv('True.csv')\n",
    "\n",
    "# Combine the datasets and shuffle\n",
    "fake_news['label'] = 0    # adds a column\n",
    "real_news['label'] = 1\n",
    "\n",
    "data = pd.concat([fake_news, real_news]).sample(frac=1.0)   # frac 1.0 => returns all the data (rows)\n",
    "\n",
    "# Write the combined data to a new CSV file\n",
    "data.to_csv('Combined.csv', index=False)\n",
    "\n",
    "print(\"Data loaded successfully and combined CSV generated!\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fkv-BogQYK1S"
   },
   "source": [
    "# Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_hH1GM2GkAK"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Replace characters that are not between a to z or A to Z with whitespace\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Convert all characters into lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove inflectional morphemes like \"ed\", \"est\", \"s\", and \"ing\" from their token stem\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "\n",
    "    # Join the processed words back into a single string\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ISfFCMaX59L"
   },
   "source": [
    "# Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loZ3t2bONI9d",
    "outputId": "561d3879-4b29-4c14-beba-5eafd812611d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "# Check if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "else:\n",
    "    device_name = 'CPU:0'\n",
    "print('Using device:', device_name)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Combined.csv')\n",
    "\n",
    "# Preprocess the text using the preprocessing function\n",
    "print(\"\\nPreprocessing data...\")\n",
    "\n",
    "data['title_preprocessed'] = data['title'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_ratio = 0.64\n",
    "val_ratio = 0.16\n",
    "test_ratio = 0.2\n",
    "\n",
    "print(\"\\nSplitting Data...\")\n",
    "\n",
    "train_data = data.sample(frac=train_ratio, random_state=42)\n",
    "remaining_data = data.drop(train_data.index)\n",
    "\n",
    "val_data = remaining_data.sample(frac=val_ratio/(val_ratio+test_ratio), random_state=42)\n",
    "test_data = remaining_data.drop(val_data.index)\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the titles and convert them into BERT input tensors\n",
    "print(\"\\nTokenizing and Converting...\")\n",
    "\n",
    "train_inputs = tokenizer(list(train_data['title_preprocessed']), truncation=True, padding='max_length', max_length=42, return_tensors='tf')\n",
    "val_inputs = tokenizer(list(val_data['title_preprocessed']), truncation=True, padding='max_length', max_length=42, return_tensors='tf')\n",
    "test_inputs = tokenizer(list(test_data['title_preprocessed']), truncation=True, padding='max_length', max_length=42, return_tensors='tf')\n",
    "\n",
    "# Convert the labels into TensorFlow tensors\n",
    "train_labels = tf.convert_to_tensor(list(train_data['label']))\n",
    "val_labels = tf.convert_to_tensor(list(val_data['label']))\n",
    "test_labels = tf.convert_to_tensor(list(test_data['label']))\n",
    "\n",
    "# Extract token tensors, segment tensors, and mask tensors from the BERT inputs\n",
    "train_token_tensors = train_inputs['input_ids']\n",
    "train_segment_tensors = train_inputs['token_type_ids']\n",
    "train_mask_tensors = train_inputs['attention_mask']\n",
    "\n",
    "val_token_tensors = val_inputs['input_ids']\n",
    "val_segment_tensors = val_inputs['token_type_ids']\n",
    "val_mask_tensors = val_inputs['attention_mask']\n",
    "\n",
    "test_token_tensors = test_inputs['input_ids']\n",
    "test_segment_tensors = test_inputs['token_type_ids']\n",
    "test_mask_tensors = test_inputs['attention_mask']\n",
    "\n",
    "# Build the BERT model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Compile the model\n",
    "print(\"\\nCompiling Model...\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Train the BERT model\n",
    "print(\"\\nTraining Model...\")\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 4\n",
    "\n",
    "history = model.fit([train_token_tensors, train_segment_tensors, train_mask_tensors], train_labels, batch_size=batch_size, epochs=num_epochs, validation_data=([val_token_tensors, val_segment_tensors, val_mask_tensors], val_labels))\n",
    "\n",
    "# Save the trained model\n",
    "print(\"\\nSaving Model...\")\n",
    "\n",
    "model.save_pretrained('/content/bertv3_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUlYf6JWXyKQ"
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FX3mw-GCNpuY",
    "outputId": "e248cd1a-dab4-4d3f-a7df-dde239bc2c51"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate([test_token_tensors, test_segment_tensors, test_mask_tensors], test_labels, batch_size=batch_size)\n",
    "print(f'Test loss: {loss * 100:.3f}%')\n",
    "print(f'Test accuracy: {accuracy * 100:.3f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CpwrDHrX1FF"
   },
   "source": [
    "# Using the Model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9l9GG-r5VHZ",
    "outputId": "9ec2c470-7f07-4f0e-b0d9-cbec5063f73c"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the saved model\n",
    "model = TFBertForSequenceClassification.from_pretrained('/content/bertv3_model')\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input text\n",
    "input_text = input(\"\\n\\nEnter News Title: \")\n",
    "\n",
    "# Preprocess the input text\n",
    "preprocessed_text = preprocess_text(input_text)\n",
    "\n",
    "print(\"Preprocessed Text:\", preprocessed_text)\n",
    "\n",
    "# Tokenize the preprocessed text\n",
    "inputs = tokenizer(preprocessed_text, truncation=True, padding='max_length', max_length=42, return_tensors='tf')\n",
    "\n",
    "# Extract input tensors\n",
    "token_tensors = inputs['input_ids']\n",
    "segment_tensors = inputs['token_type_ids']\n",
    "mask_tensors = inputs['attention_mask']\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict([token_tensors, segment_tensors, mask_tensors])\n",
    "logits = predictions.logits[0]\n",
    "probabilities = tf.nn.softmax(logits)\n",
    "predicted_label = tf.argmax(probabilities)\n",
    "\n",
    "# Print the predicted label and probabilities\n",
    "if predicted_label == 0:\n",
    "    print(\"\\n*-*-Fake News-*-*\")\n",
    "else:\n",
    "    print(\"\\n*-*-Real News-*-*\")\n",
    "\n",
    "print(\"\\nProbability of being fake: {:.2%}\".format(probabilities[0]))\n",
    "print(\"Probability of being real: {:.2%}\".format(probabilities[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ewn1IRYX7UA"
   },
   "source": [
    "BREAKING: MUSLIM OPENS FIRE ON JOURNALISTS [Video]\n",
    "\n",
    " WATCH: Wolf Blitzer Makes Republican Throw Temper Tantrum Over Trumpâ€™s Nazi Problem\n",
    "\n",
    " Boston men jailed for Trump-inspired hate crime attack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2p0PCN6b-90A",
    "outputId": "84086805-f856-4033-923c-04d22897ddaa"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
